# NanoVLM SFT (Supervised Fine-Tuning) Configuration
# This config file defines all parameters for SFT training

# Training type
training_type: "sft"

# Model configuration
model:
  model_name_or_path: "lusxvr/nanoVLM-230M-8k"  # Path to pre-trained nanoVLM checkpoint OR "initialize" to create fresh model
  use_lora: true
  lora_config:
    r: 16
    lora_alpha: 16
    lora_dropout: 0.1
    target_modules: ['down_proj', 'o_proj', 'k_proj', 'q_proj', 'gate_proj', 'up_proj', 'v_proj']
    use_dora: false
    init_lora_weights: "gaussian"
  model_type: "nanovlm"  # Model architecture type
  initialize_from_config: false  # Set to true if using "initialize" to create fresh model from config

# Dataset configuration
dataset:
  train_data_path: "lmms-lab/multimodal-open-r1-8k-verified"  # HuggingFace dataset repo ID or path
  train_split: "train[:5%]"  # Split name for training data (default: "train")
  eval_data_path: null  # HuggingFace dataset repo ID (optional, if null will split train data 80/20)
  eval_split: "test"  # Split name for evaluation data (default: "test")
  dataset_format: "vqa"  # Dataset format: 'vqa' for image-text pairs
  max_samples: null  # Maximum number of samples to use (null = use all)
  preprocessing_fn: "dataset_preprocessing_sft.py"  # Path to preprocessing functions file (optional)

# Training hyperparameters
training:
  output_dir: "./sft_output"  # Directory to save checkpoints and outputs
  num_train_epochs: 3
  max_steps: -1  # -1 means use num_train_epochs
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  
  # Learning rates for different components
  learning_rate: 5e-5  # Default learning rate
  lr_mp: 5e-3  # Modality Projector learning rate
  lr_vision_backbone: 5e-5  # Vision encoder learning rate (0 = frozen)
  lr_language_backbone: 5e-5  # Language model learning rate (0 = frozen)
  lr_lora: 5e-4  # LoRA adapter learning rate (if using PEFT)
  
  # Optimization
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Learning rate schedule
  lr_scheduler_type: "cosine"
  
  # Mixed precision training
  bf16: true  # Use bfloat16 precision (requires compatible GPU)
  fp16: false  # Use float16 precision (alternative to bf16)
  
  # Checkpointing and logging
  save_strategy: "steps"
  save_steps: 500
  eval_strategy: "steps"
  eval_steps: 5
  logging_steps: 5
  save_total_limit: 3  # Keep only 3 most recent checkpoints
  
  # Seed and reproducibility
  seed: 42
  
  # Device configuration
  device: "cuda"  # 'cuda' or 'cpu'
  n_gpu: 1  # Number of GPUs to use

# Model save configuration
save:
  save_model_path: "./sft_output/final_model"  # Path to save final model
  save_tokenizer: true
  save_image_processor: true

# Logging configuration
logging:
  use_wandb: false  # Set to true to log to Weights & Biases
  wandb_project: "nanovlm-sft"
  wandb_entity: null  # Your W&B entity/team name
  report_to: ["tensorboard"]  # Options: ["tensorboard", "wandb"]
