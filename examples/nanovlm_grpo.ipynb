{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/RL-VLM-Lab\n",
      "Project root: /workspace/RL-VLM-Lab\n",
      "NanoVLM root: /workspace/RL-VLM-Lab/nanovlm\n",
      "Current directory: /workspace/RL-VLM-Lab/nanovlm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "%cd ./nanovlm-lab/\n",
    "project_root = Path.cwd()\n",
    "\n",
    "nanovlm_root = project_root / \"nanovlm\"\n",
    "\n",
    "# Add paths in the correct order\n",
    "if str(nanovlm_root) not in sys.path:\n",
    "    sys.path.insert(0, str(nanovlm_root))  # Add nanoVLM FIRST\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))  # Then add project root\n",
    "\n",
    "# Change working directory to nanovlm so relative imports work\n",
    "os.chdir(nanovlm_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"NanoVLM root: {nanovlm_root}\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OI1KCTZIxEkh",
    "outputId": "c8882042-0b50-4fe6-ed24-83148397e354",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_id = 'lmms-lab/multimodal-open-r1-8k-verified'\n",
    "dataset = load_dataset(dataset_id, split='train[:5%]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N6v-TqppyfTU"
   },
   "outputs": [],
   "source": [
    "# Then convert to RGB\n",
    "def convert_to_rgb(example):\n",
    "    image = example[\"image\"]\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    example[\"image\"] = image\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(convert_to_rgb)\n",
    "\n",
    "def convert_fn(batch):\n",
    "    texts_list = []\n",
    "\n",
    "    for q, ans in zip(batch[\"problem\"], batch[\"solution\"]):\n",
    "\n",
    "        user_text = (\n",
    "            \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "            \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "            \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "            \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    "        )\n",
    "\n",
    "        assistant_text = f\"Answer: {ans}\"\n",
    "\n",
    "        # Single-element list, with a single dictionary containing both messages\n",
    "        texts_list.append([\n",
    "            {\n",
    "                \"user\": user_text+\". \"+q,\n",
    "                \"assistant\": assistant_text\n",
    "            }\n",
    "        ])\n",
    "\n",
    "    return {\"texts\": texts_list}\n",
    "dataset = dataset.map(\n",
    "    convert_fn,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=716x200>,\n",
       " 'problem': 'Based on the image, determine the constant term after combining all the polynomial expressions representing the side lengths of the triangle. Choose the correct answer from the options provided.\\n\\nChoices:\\nA. 3\\nB. 5\\nC. 8\\nD. 13',\n",
       " 'solution': \"<think>Let's examine the polynomial expressions given for each side of the triangle. The side labeled \\\\(4x^2 + x\\\\) does not have a constant term. The side labeled \\\\(2x + 3\\\\) has a constant term of 3. The side labeled \\\\(4x^3 + 2x^2 + 5\\\\) has a constant term of 5. To find the total constant term, we need to add the constant terms from these expressions. So, we add 3 and 5 together. 3 + 5 = 8</think>\\n\\n<answer>The correct answer is C</answer>\",\n",
       " 'original_question': 'According to the question shown in the image, please first perform reasoning, then finally select the right answer from the choices, e.g., Answer: xxx.\\nQuestion: Based on the image, find the constant term after combining the side lengths.\\nChoices:\\nA. 3\\nB. 5\\nC. 8\\nD. 13',\n",
       " 'original_answer': 'The constant terms from the sides $2 x + 3$ and $4 x^3 + 2 x^2 + 5$ are combined as $3 + 5 = 8$. So the answer is C\\nAnswer: C',\n",
       " 'texts': [{'assistant': \"Answer: <think>Let's examine the polynomial expressions given for each side of the triangle. The side labeled \\\\(4x^2 + x\\\\) does not have a constant term. The side labeled \\\\(2x + 3\\\\) has a constant term of 3. The side labeled \\\\(4x^3 + 2x^2 + 5\\\\) has a constant term of 5. To find the total constant term, we need to add the constant terms from these expressions. So, we add 3 and 5 together. 3 + 5 = 8</think>\\n\\n<answer>The correct answer is C</answer>\",\n",
       "   'user': 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Based on the image, determine the constant term after combining all the polynomial expressions representing the side lengths of the triangle. Choose the correct answer from the options provided.\\n\\nChoices:\\nA. 3\\nB. 5\\nC. 8\\nD. 13'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q0vNuVjT-jMd"
   },
   "outputs": [],
   "source": [
    "def extract_prompt(example):\n",
    "    # safe extraction in case texts is missing or empty\n",
    "    if example.get('texts') and len(example['texts']) > 0:\n",
    "        return {'prompt': example['texts'][0].get('user', '')}\n",
    "    return {'prompt': ''}\n",
    "\n",
    "train_dataset = train_dataset.map(extract_prompt,num_proc=4)\n",
    "test_dataset = test_dataset.map(extract_prompt,num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gQ86okNr-muj"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns([\"problem\",\"original_question\",\"texts\"])\n",
    "test_dataset = test_dataset.remove_columns([\"problem\",\"original_question\",\"texts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DH5R0F9Uip9S",
    "outputId": "cc382b96-d5dc-49ee-8bfe-ddcb3ec8dba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from data.datasets import VQADataset\n",
    "from data.collators import VQACollator\n",
    "from data.data_utils import synchronized_dataloader_step\n",
    "from data.advanced_datasets import ConstantLengthDataset\n",
    "from data.processors import get_image_processor, get_tokenizer\n",
    "\n",
    "import models.config as config\n",
    "from models.vision_language_model import VisionLanguageModel\n",
    "\n",
    "# Libraries\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets, get_dataset_config_names\n",
    "\n",
    "#Otherwise, the tokenizer will throw a warning\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "# To reload the modules if you change something in the code\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resize to max side len: True\n",
      "trainable params: 4,331,520 || all params: 232,395,456 || trainable%: 1.8639\n"
     ]
    }
   ],
   "source": [
    "from nanovlm.data.processors import get_tokenizer, get_image_processor\n",
    "from nanovlm.models.vision_language_model import VisionLanguageModel\n",
    "from rlvlm.nanovlm_grpo_trainer import NanoVLMGRPOTrainer, create_nanovlm_grpo_dataset,NanoVLMGRPOConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load model\n",
    "model = VisionLanguageModel.from_pretrained(\"lusxvr/nanoVLM-230M-8k\")\n",
    "\n",
    "# Get processors\n",
    "tokenizer = get_tokenizer(model.cfg.lm_tokenizer, model.cfg.vlm_extra_tokens, model.cfg.lm_chat_template)\n",
    "image_processor = get_image_processor(model.cfg.max_img_size, model.cfg.vit_img_size, model.cfg.resize_to_max_side_len)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n",
    "    use_dora=False,\n",
    "    init_lora_weights=\"gaussian\"\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DADILuPC9c30"
   },
   "outputs": [],
   "source": [
    "# Reward functions\n",
    "import re\n",
    "from typing import Optional, List\n",
    "def format_reward(completions: List[str], **kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Reward function that checks if the completion has the correct format.\n",
    "    \n",
    "    Expects format: <think>...</think> followed by <answer>...</answer>\n",
    "    Flexible with whitespace and newlines.\n",
    "    \n",
    "    Args:\n",
    "        completions: List of completion strings\n",
    "        **kwargs: Additional arguments (ignored)\n",
    "    \n",
    "    Returns:\n",
    "        List of rewards (1.0 for correct format, 0.0 otherwise)\n",
    "    \"\"\"\n",
    "    # Pattern allows flexible whitespace between tags\n",
    "    pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n",
    "    rewards = []\n",
    "    \n",
    "    for content in completions:\n",
    "        if re.search(pattern, content, re.DOTALL):\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n",
    "def accuracy_reward(\n",
    "    completions: List[str],\n",
    "    answer: Optional[str] = None,\n",
    "    partial_credit: bool = True,\n",
    "    partial_credit_threshold: float = 0.5,\n",
    "    **kwargs\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Reward function for answer accuracy using regex matching.\n",
    "    \n",
    "    Scoring:\n",
    "    - 1.0: Answer extracted from <answer> tags matches expected answer exactly\n",
    "    - 0.5: Expected answer is found within the extracted answer (partial credit)\n",
    "    - 0.0: No match or answer not found\n",
    "    \n",
    "    Args:\n",
    "        completions: List of completion strings\n",
    "        answer: Expected answer (from dataset) - can be just the letter (e.g., \"C\") or full text\n",
    "        partial_credit: Whether to give partial credit if answer is found within response (default: True)\n",
    "        partial_credit_threshold: Reward value for partial matches (default: 0.5)\n",
    "        **kwargs: Additional arguments (ignored)\n",
    "    \n",
    "    Returns:\n",
    "        List of rewards (0.0, 0.5, or 1.0)\n",
    "    \"\"\"\n",
    "    if completions is None or answer is None:\n",
    "        return [0.0] * len(completions) if completions else []\n",
    "    \n",
    "    rewards = []\n",
    "    answer_str = str(answer).strip().lower()\n",
    "    \n",
    "    # Extract just the letter/answer from the expected answer\n",
    "    # Handle cases like \"The answer is C\" or \"Answer: C\" or full solution text ending with answer\n",
    "    # Look for the pattern \"answer:\" or \"answer is\" followed by a letter\n",
    "    answer_match_expected = re.search(r\"(?:answer\\s*(?:is|:)?\\s*)([a-d])\", answer_str)\n",
    "    if answer_match_expected:\n",
    "        answer_clean = answer_match_expected.group(1).lower()\n",
    "    else:\n",
    "        # If no \"answer:\" pattern found, look for the last letter [a-d] in the string\n",
    "        letters = re.findall(r\"[a-d]\", answer_str)\n",
    "        if letters:\n",
    "            answer_clean = letters[-1].lower()  # Take the last occurrence\n",
    "        else:\n",
    "            answer_clean = answer_str\n",
    "    \n",
    "    for completion in completions:\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Extract answer from <answer>...</answer> tags\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", completion, re.DOTALL)\n",
    "        \n",
    "        if answer_match:\n",
    "            extracted_answer = answer_match.group(1).strip().lower()\n",
    "            \n",
    "            # Check for exact match (single letter or full answer)\n",
    "            if extracted_answer == answer_clean:\n",
    "                reward = 1.0\n",
    "            # Check for partial match (answer found within extracted answer)\n",
    "            elif partial_credit and answer_clean in extracted_answer:\n",
    "                reward = partial_credit_threshold\n",
    "        \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7HEOzs1q9wF9"
   },
   "outputs": [],
   "source": [
    "train_dataset = create_nanovlm_grpo_dataset(\n",
    "    dataset=train_dataset,\n",
    "    prompt_column=\"prompt\",\n",
    "    image_column=\"image\",\n",
    ")\n",
    "test_dataset = create_nanovlm_grpo_dataset(\n",
    "    dataset=test_dataset,\n",
    "    prompt_column=\"prompt\",\n",
    "    image_column=\"image\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "W6xTMtzZA64E"
   },
   "outputs": [],
   "source": [
    "grpo_config = NanoVLMGRPOConfig(\n",
    "    output_dir=\"./nanovlm-grpo-output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-6,\n",
    "    num_train_epochs=2,\n",
    "    num_generations=3,  # Generate 4 completions per prompt\n",
    "    max_prompt_length=2048,\n",
    "    max_completion_length=512,\n",
    "    beta=0.1,  # KL penalty\n",
    "    temperature=1.0,\n",
    "    logging_steps=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=3,\n",
    "    log_completions=False,\n",
    "    # DAPO-specific settings\n",
    "    loss_type=\"grpo\",\n",
    "    importance_sampling_level=\"token\",\n",
    "    mask_truncated_completions=True,\n",
    "    fp16=True,\n",
    "    bf16=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Zd6dzpnxA7rK"
   },
   "outputs": [],
   "source": [
    "trainer = NanoVLMGRPOTrainer(\n",
    "    model=peft_model,\n",
    "    reward_funcs=[format_reward,accuracy_reward],\n",
    "    args=grpo_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    image_processor=image_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Fr7Cxc9pFLp4"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZfO_cV0YBQ7w",
    "outputId": "adf85e04-c672-40a9-d2ed-246add728350"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='308' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/308 00:32 < 1:21:10, 0.06 it/s, Epoch 0.02/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/77 01:50 < 08:08, 0.13 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Resize to max side len: True\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from PIL import Image\n",
    "from peft import PeftModel\n",
    "\n",
    "from models.vision_language_model import VisionLanguageModel\n",
    "from data.processors import get_tokenizer, get_image_processor, get_image_string\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "source = \"./nanovlm-grpo-output\"\n",
    "print(f\"Loading weights from: {source}\")\n",
    "\n",
    "# 1. Load the base model\n",
    "base_model = VisionLanguageModel.from_pretrained(\"lusxvr/nanoVLM-230M-8k\").to(device)\n",
    "\n",
    "# 2. Load LoRA adapters on top\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    source,  # Your output_dir with adapter_config.json\n",
    ")\n",
    "\n",
    "# 3. Get tokenizer and image processor\n",
    "tokenizer = get_tokenizer(model.cfg.lm_tokenizer, model.cfg.vlm_extra_tokens, model.cfg.lm_chat_template)\n",
    "image_processor = get_image_processor(model.cfg.max_img_size, model.cfg.vit_img_size, False)\n",
    "\n",
    "# 4. Set to eval mode\n",
    "model.eval()\n",
    "# Now you can use model for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "img = test_dataset[idx][\"image\"].convert('RGB')\n",
    "processed_image, splitted_image_ratio = image_processor(img)\n",
    "if not hasattr(tokenizer, \"global_image_token\") and splitted_image_ratio[0]*splitted_image_ratio[1] == len(processed_image) - 1:\n",
    "    # If the tokenizer doesn't have a global image token, but the processor generated it, remove it\n",
    "    processed_image = processed_image[1:]\n",
    "\n",
    "image_string = get_image_string(tokenizer, [splitted_image_ratio], model.cfg.mp_image_token_length)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": image_string + test_dataset[idx]['prompt']}]\n",
    "encoded_prompt = tokenizer.apply_chat_template([messages], tokenize=True, add_generation_prompt=True)\n",
    "tokens = torch.tensor(encoded_prompt).to(device)\n",
    "img_t = processed_image.to(device)\n",
    "\n",
    "print(\"\\nInput:\\n \", test_dataset[idx]['prompt'], \"\\n\\nOutput:\")\n",
    "\n",
    "for i in range(3):\n",
    "    gen = model.generate(tokens, img_t, max_new_tokens=512)\n",
    "    out = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "    print(f\"  >> Generation {i+1}: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "runtime_attributes": {
    "runtime_version": "2025.07"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
